{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f91210d",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594525d",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes i Atribució d'autors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce1485",
   "metadata": {},
   "source": [
    "En aquesta segona part es programarà un classificador, que donat un document el categoritzarà per atribuir-lo al seu possible autor. Els textos provindran del dataset amb el qual s'ha treballat a la primera part.\n",
    "\n",
    "\n",
    "**Què s’ha de fer?**\n",
    "\n",
    "Volem classificar textos literaris segons el seu escriptor. \n",
    "A partir de tots els textos que tenim, desenvoluparem un classificador probabilístic del tipus Naive Bayes que ens permeti identificar a quin autor pertany llibre o text segons les característiques triades.\n",
    "\n",
    "\n",
    "**Quina és la idea del sistema de classificació que s’ha de desenvolupar?**\n",
    "\n",
    "El classificador és un concepte de l'aprenentatge automàtic supervisat. \n",
    "L'objectiu del classificador és donat unes característiques que descriuen els objectes que es volen classificar indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. \n",
    "El procés de classificació consta de dues parts: \n",
    "(a) el procés d'aprenentatge i \n",
    "(b) el procés d'explotació o testeig. \n",
    "El procés d'aprenentatge rep exemples de parelles $(x,y)$ on $x$ són les característiques, usualment nombres reals, i $y$ és la categoria a la que pertanyen. \n",
    "Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció $\\hat{y}=h(x)$ que donada una $x$ aconsegueixi que $\\hat{y}=y$. Per altra banda el procés de testeig aplica la funció $h(x)$ apresa a l'entrenament a una nova descripció per veure quina categoria li correspon.\n",
    "\n",
    "\n",
    "**Classificació i llenguatge natural**\n",
    "\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. \n",
    "Una de les representacions més simples per tal de descriure un text és la representació *bag-of-words*.\n",
    "Aquesta representació converteix un text en un conjunt de $N$ paraules. \n",
    "Consisteix en seleccionar un conjunt d'$N$ paraules i per cada paraula comptar quants cops apareix en el text. \n",
    "Una versió alternativa d'aquest procés pot ser simplement indicar si apareix o no en el text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91a5f0",
   "metadata": {},
   "source": [
    "## Abans de començar\n",
    "\n",
    "\n",
    "**\\+ No es poden modificar les definicions de les funcions donades, ni canviar els noms de les variables i paràmetres ja donats**\n",
    "\n",
    "\n",
    "**\\+ En les funcions, s'especifica què serà i de quin tipus cada un dels paràmetres, cal respectar-ho**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d74030",
   "metadata": {},
   "source": [
    "Executeu les següents dues cel·les: inclouen alguns imports necessaris, així com la funció `tokenize` per separar paraules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce783df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from glob import glob\n",
    "import os\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67486150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taller', 'dels', 'nous', 'usos', 'de', 'la', 'informàtica']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, lowercase=True):\n",
    "    text = text.lower() if lowercase else text\n",
    "    for match in re.finditer(r\"\\w+(\\.?\\w+)*\", text):\n",
    "        yield match.group()\n",
    "\n",
    "list(tokenize('taller dels nous usos de la informàtica'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d9c4b",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "Per fer l'exercici més interessant, generalitzarem una mica la representació de bag-of-words per acceptar n-grames. Si tenim la frase \"taller de nous usos de la informàtica\", els 1-grames són cadascuna de les paraules individuals. En canvi, els bigrames (2-grames) són les parelles de paraules consecutives:\n",
    "\n",
    "```\n",
    "\"taller de\", \"de nous\", \"nous usos\", \"usos de\", \"de la\", \"la informàtica\".\n",
    "```\n",
    "\n",
    "Els n-grames guarden més informació d'un text i de l'estil de qui l'ha escrit.\n",
    "\n",
    "**Exercici 1**\n",
    "\n",
    "Implementeu la funció que donat un iterable ens dóna la seva llista de n-grames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ba12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(iterable, ngram_range=(1, 1)):\n",
    "    \"\"\"Donat un iterable (típicament, serà una llista de paraules),\n",
    "    retornar els seus ngrames, des dels de menor longitud als de major longitud,\n",
    "    segons ngram_range.\n",
    "    \n",
    "    Input:\n",
    "        iterable: una llista de paraules\n",
    "        ngram_range: tupla amb dos elements: la longitud mínima i la màxima dels \n",
    "            ngrames que volem\n",
    "    \n",
    "    Returns:\n",
    "        Llista amb tots els ngrames generats\n",
    "    \n",
    "    \"\"\"\n",
    "    min_n, max_n = ngram_range\n",
    "    # Passem els iterables a tuples perque siguin 'hashable'\n",
    "    if isinstance(iterable, (list, set)):\n",
    "        iterable = tuple(iterable)\n",
    "    ngrams = []\n",
    "    \n",
    "    # part a implementar per l'alumne\n",
    "    # EL VOSTRE CODI AQUÍ\n",
    "    for i in range(min_n, max_n +1):#we start at 1 cause 0 would be nothing, we end at max_n + 1 so it includes the last\n",
    "        temp=zip(*[iterable[j:] for j in range(0,i)]) #where i is the number of grams to be generated\n",
    "        ngrams = ngrams + [ngram for ngram in temp] #concatenate them all\n",
    "        \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a414c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Els següents tests haurien de donar `True` si ho heu implementat correctament:\n",
    "ngrams(\n",
    "    list(tokenize(\"taller de nous usos de la informàtica\")), \n",
    "    ngram_range=(1, 2)\n",
    ") == [\n",
    "    ('taller',), ('de',), ('nous',), ('usos',), ('de',), ('la',), ('informàtica',), \n",
    "    ('taller', 'de'), ('de', 'nous'), ('nous', 'usos'), \n",
    "    ('usos', 'de'), ('de', 'la'), ('la', 'informàtica')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc640e38",
   "metadata": {},
   "source": [
    "### Processament de les dades: generació de features amb tokens i amb n-grames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047035b",
   "metadata": {},
   "source": [
    "El primer que farem és processar cadascun dels textos que volem considerar. Per cada document, el convertirem a una llista de paraules i en generarem n-grames. Després, seleccionarem els N n-grames més comuns. Aquests seran les \"features\" del document. En altres paraules, treballem amb la hipòtesi que els n-grames més comuns determinen l'estil de l'autor. \n",
    "\n",
    "*Recordeu que podeu utilitzar la classe `Counter`.*\n",
    "\n",
    "**Exercici 2.** Implementeu la funció `create_features` tal com s'ha descrit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2799b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(filenames, N=10, ngram_range=(1,1)):\n",
    "    \"\"\"A partir d'una llista d'arxius i un rang d'ngrames, creem dos \n",
    "    llistes: un de noms d'autor, i un de llistes de ngrames.\n",
    "        \n",
    "    Input:\n",
    "        filenames: llista de documents a processar\n",
    "        ngram_range: n-grames que volem crear\n",
    "        \n",
    "    Returns:\n",
    "        X: llista dels N ngrames més comuns de cada document\n",
    "        y: llista d'autors (etiquetes) de cada document\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        author = (os.path.basename(filename).split('-'))[0]\n",
    "        words = list(tokenize(open(filename, encoding=\"utf-8\").read()))\n",
    "    \n",
    "        # part a implementar per l'alumne\n",
    "        # EL VOSTRE CODI AQUÍ\n",
    "        \n",
    "        y.append(author)\n",
    "        temp = ngrams(words, ngram_range) #we get the ngrams\n",
    "        #since most_common(N) returns a tuple with the ngram and the frequency of apperance\n",
    "        #instead of just appending Counter(temp).most_common(N), we break the tuple into i and j\n",
    "        #where i is the ngram and j is the frequency, and we just append the ngram.\n",
    "        common_ngrams = []\n",
    "        for i,j in Counter(temp).most_common(N):\n",
    "            common_ngrams.append(i)\n",
    "        X.append(common_ngrams) #we append the N most common ngrams\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38adc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen [('to',), ('the',), ('and',), ('of',), ('i',), ('a',), ('it',), ('her',), ('was',), ('she',)]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = create_features(glob('data/gutenberg/training/*.txt'), N=10)\n",
    "print(train_y[0], train_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76835c7a",
   "metadata": {},
   "source": [
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dd980",
   "metadata": {},
   "source": [
    "### El classificador Naïve Bayes\n",
    "\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. \n",
    "En aquest lliurament farem servir el classificador Naïve Bayes. \n",
    "Aquest classificador forma part de la família de classificadors probabilístics. \n",
    "La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. \n",
    "La decisió final correspon a la categoria amb més probabilitat. \n",
    "\n",
    "\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per trobar la probabilitat condicionada: \n",
    "$$ p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "d'on podem extreure que: \n",
    "$$ p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "\n",
    "En molts casos $p(y)$ i $p(x)$ són desconeguts i es consideren equiprobables. Per nosaltres, p(y) serà la quantitat de vegades que hem vist l'autor y dividit pel nombre total de documents. Aquesta distribució de probabilitat dels autors de vegades s'anomena la \"prior\".\n",
    "Per tant, la decisió es simplifica a:\n",
    "$$ p(y|x) = c · p(x|y)p(y).$$\n",
    "\n",
    "\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. \n",
    "Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. \n",
    "Considerem $x=(x_1, \\cdots, x_n)$ un conjunt d'$n$ variables aleatòries. \n",
    "Naïve Bayes assumeix que totes elles són independents entre elles i per tant podem escriure:\n",
    "$$p(x_1,x_2,...,x_n | y) = p(x_1|y)p(x_2|y)...p(x_n|y).$$\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que el tweet descrit pel vector de característiques (\"in the\", \"of the\", \"and\") sigui de la classe \"blake\" és proporcional al producte de la probabilitat que la \"in the\" del vector aparegui en els textos de \"blake\" per la probabilitat que \"of the\" hi aparegui, per la probabilitat que \"and\" hi aparegui.\n",
    "\n",
    "\n",
    "La fórmula final queda doncs\n",
    "$$ p(y|x_1,x_2,\\dots,x_N) \\propto p(x_1|y)p(x_2|y)\\cdots p(x_n|y)p(y)$$\n",
    "on $y$ és un autor i $x_1,\\dots,x_N$ és un conjunt de paraules o n-grames.\n",
    "\n",
    "\n",
    "**Estimant les probabilitats marginals condicionades**\n",
    "\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. \n",
    "Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. \n",
    "Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada autor. \n",
    "Aquest càlcul es fa dividint el nombre de textos de l'autor en què apareix la paraula pel nombre total de textos d'aquell autor. \n",
    "\n",
    "En general:\n",
    "$$p(x = \\text{\"father\"} | y = C)= \\frac{A}{B} $$\n",
    "on A és el número de textos de l'autor C on hi apareix la paraula 'father' i B és el número total de textos de l'autor C.\n",
    "\n",
    "\n",
    "### Punts dèbils:\n",
    "\n",
    "**El problema de la probabilitat 0**\n",
    "\n",
    "Si us hi fixeu bé, la probabilitat pot ser 0 !! \n",
    "Això vol dir, que si en el text hi apareix una paraula que no hem vist abans, no pot ser classificada per cap autor.\n",
    "El que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. \n",
    "Una de les possibles solucions es fer servir la correcció de Laplace. \n",
    "Seguint l'exemple anterior la correcció de Laplace és\n",
    "$$p(x= \\text{\"father\"} | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "on M és el nombre de categories, i A, B, C són com abans.\n",
    "\n",
    "**El problema del \"underflow\"**\n",
    "\n",
    "La funció que hem de calcular en el Naive Bayes és un producte. \n",
    "El nombre de característiques del vector és el nombre de termes del producte. \n",
    "Aquests nombres són iguals o menors a 1, si els multipliquem tots entre ells el resultat serà massa petit per a representar-lo en un float i el càlcul acabarà sent reduït a zero. \n",
    "Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions. Dit d'una altra manera,\n",
    "\n",
    "$$\\log(p(y|x_1,x_2,\\dots,x_N)) \\propto \\log(p(y)) + \\log(p(x_1|y)) + \\log(p(x_2|y))+\\cdots+ \\log(p(x_n|y))$$\n",
    "\n",
    "### Classificar:\n",
    "\n",
    "Donat un llistat de n-grames $x=(x_1,...,x_n)$, per classificar el corresponent text calcularem la probabilitat de pertànyer a cadascun dels autors:\n",
    "\n",
    "$$p(\\text{austen}|x) = p(\\text{austen})\\prod_{i=1}^np(x_i|\\text{austen})$$\n",
    "$$\\cdots$$\n",
    "$$p(\\text{shakespeare}|x) = p(\\text{shakespeare})\\prod_{i=1}^np(x_i|\\text{shakespeare})$$\n",
    "\n",
    "I finalment, el text és de l'autor de probabilitat màxima. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2d2db",
   "metadata": {},
   "source": [
    "**Exercici 3.** Implementeu la classe `NaiveBayesLearner`. Us donem el mètode `fit`, que incorpora vectors de features i etiquetes per calcular les probabilitats. Heu d'implementar tres mètodes: \n",
    "\n",
    "- prior\n",
    "- probability\n",
    "- predict\n",
    "\n",
    "Les cel·les posteriors us permeten testejar la vostra implementació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d12682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \"\"\"Classificador Naive Bayes\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrenament del Naive Bayes Classifier.\n",
    "        \n",
    "        Input:\n",
    "            X: llista de features (cada feature serà un vector de n-grames)\n",
    "            y: llista d'etiquetes (autors). L'ordre de X i de y ha de correspondre\n",
    "        \"\"\"\n",
    "        self.C = Counter(y)\n",
    "        self.N = defaultdict(Counter)\n",
    "        for x, y_x in zip(X, y):\n",
    "            self.N[y_x] += Counter(x)\n",
    "        self.V = len(set(x for y_x in self.N for x in self.N[y_x]))\n",
    "        \n",
    "        \n",
    "    def prior(self, y):\n",
    "        \"\"\"Donada una etiqueta y, retorna p(y), \n",
    "        segons la informació que tenim a self.C\n",
    "        \n",
    "        Input:\n",
    "            y: string, una etiqueta (autor)\n",
    "        \n",
    "        Returns:\n",
    "            p(y), float entre 0 i 1\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        p(y), prior, is the number of times we have seen an author / total number of documents\n",
    "        \"\"\"\n",
    "        authors_seen = self.C[y] #given the y is a tag/author\n",
    "        all_docs = sum(self.C.values())\n",
    "        p_y = authors_seen / all_docs\n",
    "        return p_y\n",
    "\n",
    "        \n",
    "    def probability(self, x, y):\n",
    "        \"\"\"Dona la probabilitat que ens aparegui l'n-grama x si l'etiqueta és y: p(x|y)\n",
    "        \n",
    "        Input:\n",
    "            x: un n-grama\n",
    "            y: una etiqueta (autor)\n",
    "        \n",
    "        Returns:\n",
    "            p(x|y), float entre 0 i 1\n",
    "        \"\"\"\n",
    "        #Where A is the number of texts of author C where the word x, an ngram, appears \n",
    "        #B is the total number of texts of author C, which is the same as saying n times an author appears.\n",
    "        #M is the number of categories\n",
    "        \n",
    "        num = self.N[y][x] + 1 #A + 1\n",
    "        den = self.C[y] + len(self.C.keys()) #B + M\n",
    "        prob = num / den\n",
    "        return prob\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Prediu l'etiqueta de l'exemple x (un vector de n-grames).\n",
    "        \n",
    "        Input:\n",
    "            x: llista de n-grames\n",
    "            \n",
    "        Returns:\n",
    "            L'etiqueta predita per x, d'acord amb la fórmula del Naive Bayes\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = None\n",
    "        #if we use 0 or 0.001 it wont work, so we need to use a very small number\n",
    "        alpha = float(\"-inf\") #we use a starting probability, as we calculate the probability, we update alpha until we get the biggest\n",
    "        catgs = self.C.keys() #we get the categories\n",
    "        \n",
    "        for y in catgs: #where y is an author \n",
    "            \n",
    "            p_y = log(self.prior(y)) #we get the p(y) for each author\n",
    "            for j in x:\n",
    "                p_y += log(self.probability(j, y)) #where j is an ngram\n",
    "                \n",
    "            if p_y > alpha: #if we find a new best probability\n",
    "                alpha = p_y #update the best one\n",
    "                prediction = y #make that author its prediction\n",
    "        \n",
    "        return prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4c11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "train_X, train_y = create_features(glob('data/gutenberg/training/*.txt'), N=N, ngram_range=(1,1))\n",
    "test_X, test_y = create_features(glob('data/gutenberg/testing/*.txt'), N=N, ngram_range=(1,1))\n",
    "\n",
    "bayes = NaiveBayes()\n",
    "bayes.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868bc32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen chesterton\n",
      "whitman milton\n",
      "whitman shakespeare\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for x, y in zip(test_X, test_y):\n",
    "    x_counter = Counter(x)\n",
    "\n",
    "    y_pred = bayes.predict(x)\n",
    "    predictions.append(y_pred)\n",
    "    print(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84fa1d",
   "metadata": {},
   "source": [
    "### Avaluació de resultats: accuracy, precision, recall\n",
    "\n",
    "En aquesta darrera secció avaluarem el nostre classificador i provarem de trobar un bon conjunt de paràmetres (mida dels ngrames, nombre N de ngrames més comuns als vectors de features) per a fer-lo millor.\n",
    "\n",
    "Suposem que tenim un seguit d'atribucions correctes $t_1,\\dots,t_n$ i una llista de prediccions d'autors $y_1,\\dots,y_n$. El més senzill que podem fer per saber si ho hem fet és mesurar l'*accuracy*:\n",
    "\n",
    "$$\\operatorname{accuracy}\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\begin{cases}1 & \\text { si } y_{i} \\text { és igual a } t_{i} \\\\ 0 & \\text { en cas que no }\\end{cases}.$$\n",
    "\n",
    "**Exercici 4.** Implementeu la funció d'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbc09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_labels, true_labels):\n",
    "    \"\"\"Càlcul de la funció d'accuracy\n",
    "    \n",
    "    Input:\n",
    "        pred_labels: llista d'etiquetes predites\n",
    "        true_labels: llista d'etiquetes correctes\n",
    "        \n",
    "    Returns:\n",
    "        accuracy(pred_labels)\n",
    "    \"\"\"\n",
    "    num_predictions = len(pred_labels)\n",
    "    matches = 0\n",
    "    for i in range(num_predictions):\n",
    "        if pred_labels[i] == true_labels[i]:\n",
    "            matches += 1 \n",
    "    accuracy = matches / num_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357eb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(predictions, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a00e07",
   "metadata": {},
   "source": [
    "Aquesta mesura de l'accuracy no ens diu massa cosa, ja que estem fent molt poques comprovacions. \n",
    "\n",
    "Una bona forma de veure com funcionaria el nostre classificador davant de dades sobre les quals no s'ha entrenat és fer servir l'estratègia n-fold. Aquesta estratègia testeja el classificador amb una partició de les dades d'entrenament i fa l'entrenament sobre la resta de dades que hem exclòs. Aquest procés d'exclusió es repeteix per cadascún dels *folds de les dades d'entrenament. El nombre de folds determina quantes particions hem de fer, i per tant, les dades que hi ha en el conjut de test. \n",
    "\n",
    "Per exemple, en un 5-fold validation, es fan 5 particions, les dades de test són un cinquè de les dades i l'entrenament es fa amb els quatre cinquens restants. El percentatge d'errors fent servir aquesta estratègia permet comparar classificadors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a8002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X, y, start, end):\n",
    "    \"\"\"Funció per entrenar i testejar un classificador. Reservem\n",
    "    X[start:end] per entrenar, fem el test sobre la resta d'exemples.\n",
    "    \n",
    "    Inputs:\n",
    "        X: llista de vectors de característiques\n",
    "        y: llista d'etiquetes de cada exemple\n",
    "        start, end: posicions per reservar a l'entrenament\n",
    "    \n",
    "    Return:\n",
    "        proporció d'exemples correctament predits pel classificador\n",
    "    \"\"\"\n",
    "    \n",
    "    train_X, train_y = X[:start] + X[end:], y[:start] + y[end:]\n",
    "    test_X, test_y = X[start:end], y[start:end]\n",
    "    \n",
    "    bayes = NaiveBayes()\n",
    "    bayes.fit(train_X, train_y)\n",
    "    predictions = []\n",
    "    for x, y in zip(test_X, test_y):\n",
    "        predictions.append(bayes.predict(x))\n",
    "        \n",
    "    return accuracy(predictions, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c0485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(6.23894251)\n",
    "\n",
    "def shuffle(*args, seed=None):\n",
    "    \"\"\"Desordenació aleatòria d'exemples d'entrenament (X,y),\n",
    "    respectant la concordància entre cada exemple i la seva etiqueta\n",
    "    \"\"\"\n",
    "    data = list(zip(*args))\n",
    "    random.shuffle(data, random=seed)\n",
    "    return zip(*data)\n",
    "\n",
    "def n_fold(X, y, k=10):\n",
    "    \"\"\"Validació n-fold\n",
    "    \"\"\"\n",
    "    if k is None: k = len(X)\n",
    "    n = len(X)\n",
    "    X, y = shuffle(X, y)\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        start, end = int(i * (n / k)), int((i + 1) * (n / k))\n",
    "        score = train_and_test(X, y, start, end)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "N=10\n",
    "X, y = create_features(glob('data/gutenberg/**/*.txt'), N=N, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a84722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(n_fold(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29b68f",
   "metadata": {},
   "source": [
    "**Exercici 5.** Trobeu una combinació de `N` i `ngram_range` per tal que l'accuracy mitjana del `n_fold(X, y)` sigui com a mínim del 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16dcf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = create_features(glob('data/gutenberg/**/*.txt'), N=30, ngram_range=(2,4))\n",
    "print(n_fold(X1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a26e703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = create_features(glob('data/gutenberg/**/*.txt'), N=40, ngram_range=(2,6))\n",
    "print(n_fold(X1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4929af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = create_features(glob('data/gutenberg/**/*.txt'), N=100, ngram_range=(2,7))\n",
    "print(n_fold(X1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Warning: the following code uses a loop to find the N and range values to get an accuracy of 0.8 or greater\n",
    "using random values. This could take long. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4820150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 98  ngram_range=(2,4)\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "result = 0\n",
    "while not result >= 0.8:\n",
    "\n",
    "    a = int(random.uniform(2,10))\n",
    "    b = int(random.uniform(a,10))\n",
    "    N = int(random.uniform(10,200))\n",
    "    X1, y1 = create_features(glob('data/gutenberg/**/*.txt'), N=N, ngram_range=(a,b))\n",
    "    result = n_fold(X1, y1)\n",
    "    \n",
    "print(\"N: \" + str(N) + \"  ngram_range=(\" +str(a) + \",\" + str(b) + \")\")    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6290a517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = create_features(glob('data/gutenberg/**/*.txt'), N=98, ngram_range=(2,4))\n",
    "print(n_fold(X1, y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39748671",
   "metadata": {},
   "source": [
    "We try using the same parameters to see if we can get a constant 0.8 or higher, but I dont think that makes sense since\n",
    "the shuffle function uses a random.shuffle, even if we set a seed, the seed value needs to through the random.shuffle \n",
    "function, so in any case, there is a random value being generated despite using the same seed or N and range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
